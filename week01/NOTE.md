学习笔记
1、本周学习了两种方法爬网站的方式
一、使用requests请求网页，然后使用bs4下的BeautifulSoup进行html.parser解析，然后使用find或者find_all方法找到目标元素，之后获取对应的文本值或者标签值
二、使用scrapy爬虫框架进行网页爬虫，这个框架分为好几个模块。主引擎、爬虫、调度器、下载器、管道流。本次我个人主观用到的是爬虫、管道流，其他都是工具帮助我们完成
    a、需要注意的是部分网站会进行反爬虫，我们可以在settings.py配置文件中设置user-agent、headers头部等信息，然后复制我们个人电脑访问浏览器网站时的network中的头部信息，用来模拟真实请求，骗过反爬虫
    b、管道流要生成文件，也要在settings.py配置文件中打开如下配置：
    ITEM_PIPELINES = {
        'maoyan.pipelines.MaoyanPipeline': 300,
    }
    c、xpath的相对路径要看好，弄错了就得不到对应的数据了
    d、可以通过print方法打印日志的方式来定位错误

2、其他的基础
一、yield
    这个关键字与return有区别，他是每次有结果会返回，而return是在前面所有处理全部完成后才会返回。对资源占用方面友好
二、推导式
    这个写法比平时的for循环等要精简，代码写的会好看一些，也快一些